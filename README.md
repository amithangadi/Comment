Comment toxicity detection is the process of identifying harmful, offensive, or toxic language in user-generated text. It plays a crucial role in moderating online discussions and ensuring a safer digital environment. In this project, I propose a toxicity detection framework using recurrent neural networks (RNNs) with long short-term memory (LSTM) cells. We use a dataset consisting of labeled comments, where each comment is tagged with a corresponding toxicity level (toxic or non-toxic).
Our model architecture leverages LSTM units, a type of RNN capable of capturing long-range dependencies in sequential data, making them well-suited for text analysis. Input comments undergo preprocessing steps, including tokenization and encoding as numeric vectors using techniques such as word embeddings. These embeddings are then fed into an LSTM network, which learns to extract meaningful representations of toxic and non-toxic language patterns from continuous text input.
During training, we optimize model parameters using techniques such as stochastic gradient descent (SGD) and Adam optimization while applying regularization methods like early stopping to prevent overfitting. Model performance is evaluated using metrics such as precision, recall, F1-score, and accuracy on a separate test dataset.
To provide a user-friendly interface for toxicity detection, we integrate the trained LSTM model with a Flask-based web application. Users can input comments through the web interface, and the system classifies them as toxic or non-toxic in real time. Experimental results demonstrate the effectiveness of the LSTM-based framework in accurately detecting toxic comments. The model achieves competitive performance compared to traditional machine learning approaches and shows strong generalization capability on unseen data.
This project highlights the potential of deep learning in automated content moderation, contributing to a safer and more inclusive online space.# Comment
